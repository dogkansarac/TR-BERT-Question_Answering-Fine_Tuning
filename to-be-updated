{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-bert_Q&A_turkish_finetuning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1fIJztHPrJf",
        "outputId": "31b1b0d8-365b-4990-ac20-4d09e8846bb8"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9FB_rOyeAnh"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-squad\")\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"savasy/bert-base-turkish-squad\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-UXzTSXhwEY"
      },
      "source": [
        "#1. Preparing The Data\n",
        "##1.1 Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WTGRY6oeHBk"
      },
      "source": [
        "import json\n",
        "\n",
        "def read_tquad(path):\n",
        "    # open JSON file and load intro dictionary\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    # initialize lists for contexts, questions, and answers\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    # iterate through all data in squad data\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                # check if we need to be extracting from 'answers' or 'plausible_answers'\n",
        "                if 'plausible_answers' in qa.keys():\n",
        "                    access = 'plausible_answers'\n",
        "                else:\n",
        "                    access = 'answers'\n",
        "                for answer in qa[access]:\n",
        "                    # append data to lists\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "    # return formatted data lists\n",
        "    return contexts, questions, answers\n",
        "\n",
        "# execute our read SQuAD function for training and validation sets\n",
        "train_contexts, train_questions, train_answers = read_tquad('train-v0.1.json')\n",
        "val_contexts, val_questions, val_answers = read_tquad('dev-v0.1.json')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2y3SE3-ixDT",
        "outputId": "f3edb24b-bd3e-473d-9b6f-5e6715891815"
      },
      "source": [
        "print(train_contexts[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "İslam dünyasında bilimin 16. yüzyılda hala yüksek seviyede bulunduğunu gösteren çok ilginç bir örneği deskriptif coğrafya ekolünden verebiliriz. Bize bu örneği, Avrupa’da Afrikalı Leo (Leo Africanus) olarak tanınan el-hasan b. Muhammed el-Vezzan (doğumu yaklaşık 888/1483)’dır. Fas (Fez) şehrinde büyümüş ve eğitimini almış olan Granada doğumlu bu bilgin, diplomatik hizmetler yoluyla, özellikle kuzey Afrika’da olmak üzere birçok İslam ülkesini tanıyıp bir yazar olarak coğrafya ve kartografya ile ilgileniyordu. İstanbul’dan dönüş yolculuğunda Sicilyalı korsanların eline esir düşmüş, ilk olarak Napoli’ye daha sonra Roma’ya satılıp Papa X. Leo tarafından 6.1.1520 yılında bizzat Papa’nın adıyla Giovanni Leo olarak vaftiz edilmişti. İtalya’daki ikameti sırasında İtalyanca öğrendi ve Arapça öğretti. Yazar olarak faaliyetlerini Roma ve Bologna’da devam ettirdi. Afrika coğrafyası dışında kuzey Afrikalı 30 bilginin biyografilerini içeren diğer bir eser derledi. Afrika kitabını esaretinin 6. yılı olan 1526’da İtalyan dilinde tamamladı. 935/1529 yılında Tunus’a döndü ve orada Müslüman olarak öldü.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRTDTjhki7RA",
        "outputId": "9d14c710-2ace-43ac-cae8-49a766072c39"
      },
      "source": [
        "print(train_questions[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "el-Hasan b. Muhammed el-Vezzan isimli bilgin avrupa’da nasıl tanınmaktadır ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyrXk2o0i-S5",
        "outputId": "b47e62c8-3fb2-4941-ddec-0bc2d7d60e55"
      },
      "source": [
        "print(train_answers[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'answer_start': '171', 'text': 'Afrikalı Leo'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OV22u5LjMMe"
      },
      "source": [
        "Our contexts and questions are simple strings — each corresponds to each other. The answer to each question can be found within the context.\n",
        "The answers lists are slightly different in that each item is a dictionary where the answer is contained within 'text' — and the starting position of this answer within the context is also provided in 'answer_start'.\n",
        "This is okay, but we need to train our model to find the start and end of our answer within the context — so we need to add an 'answer_end' value too:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSO_CT6ijNKA"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    # loop through each answer-context pair\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        # gold_text refers to the answer we are expecting to find in context\n",
        "        gold_text = answer['text']\n",
        "        # we already know the start index\n",
        "        start_idx = int(answer['answer_start']) ######## SEE THAT IN OUR DATASET 'answer_start' is a string, not a int. So cast it to int.\n",
        "        # and ideally this would be the end index...\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # ...however, sometimes squad answers are off by a character or two\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            # if the answer is not off :)\n",
        "            answer['answer_end'] = end_idx\n",
        "        else:\n",
        "            # this means the answer is off by 1-2 tokens\n",
        "            for n in [1, 2]:\n",
        "                if context[start_idx-n:end_idx-n] == gold_text:\n",
        "                    answer['answer_start'] = start_idx - n\n",
        "                    answer['answer_end'] = end_idx - n\n",
        "            \n",
        "# and apply the function to our two answer lists\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BihXyZt_kEh5",
        "outputId": "53381d4d-f80a-4eb0-fbfa-d802665049a7"
      },
      "source": [
        "train_answers[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer_end': 183, 'answer_start': '171', 'text': 'Afrikalı Leo'},\n",
              " {'answer_end': 296, 'answer_start': '278', 'text': 'Fas (Fez) şehrinde'},\n",
              " {'answer_end': 626,\n",
              "  'answer_start': '587',\n",
              "  'text': 'ilk olarak Napoli’ye daha sonra Roma’ya'},\n",
              " {'answer_end': 646, 'answer_start': '635', 'text': 'Papa X. Leo'},\n",
              " {'answer_end': 674, 'answer_start': '658', 'text': '6.1.1520 yılında'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "773ZB1dPkgQz"
      },
      "source": [
        "##1.2 Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8fLomh1kdga"
      },
      "source": [
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob5uFfwTw6Yl",
        "outputId": "41668e6b-b899-4168-83cd-ba389cd05dc0"
      },
      "source": [
        "train_encodings.keys()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVxz4xcVAOut",
        "outputId": "37f4184b-468b-457b-8bb0-f166d0f8f8fe"
      },
      "source": [
        "train_encodings[\"token_type_ids\"][0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "WtGXuL8ElNIT",
        "outputId": "9614b495-8cc9-4c49-d5eb-10b2e4b8e425"
      },
      "source": [
        "tokenizer.decode(train_encodings['input_ids'][0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] İslam dünyasında bilimin 16. yüzyılda hala yüksek seviyede bulunduğunu gösteren çok ilginç bir örneği deskriptif coğrafya ekolünden verebiliriz. Bize bu örneği, Avrupa ’ da Afrikalı Leo ( Leo Africanus ) olarak tanınan el - hasan b. Muhammed el - Vezzan ( doğumu yaklaşık 888 / 1483 ) ’ dır. Fas ( Fez ) şehrinde büyümüş ve eğitimini almış olan Granada doğumlu bu bilgin, diplomatik hizmetler yoluyla, özellikle kuzey Afrika ’ da olmak üzere birçok İslam ülkesini tanıyıp bir yazar olarak coğrafya ve kartografya ile ilgileniyordu. İstanbul ’ dan dönüş yolculuğunda Sicilyalı korsanların eline esir düşmüş, ilk olarak Napoli ’ ye daha sonra Roma ’ ya satılıp Papa X. Leo tarafından 6. 1. 1520 yılında bizzat Papa ’ nın adıyla Giovanni Leo olarak vaftiz edilmişti. İtalya ’ daki ikameti sırasında İtalyanca öğrendi ve Arapça öğretti. Yazar olarak faaliyetlerini Roma ve Bologna ’ da devam ettirdi. Afrika coğrafyası dışında kuzey Afrikalı 30 bilginin biyografilerini içeren diğer bir eser derledi. Afrika kitabını esaretinin 6. yılı olan 1526 ’ da İtalyan dilinde tamamladı. 935 / 1529 yılında Tunus ’ a döndü ve orada Müslüman olarak öldü. [SEP] el - Hasan b. Muhammed el - Vezzan isimli bilgin avrupa ’ da nasıl tanınmaktadır? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_eHzmc-nRAu"
      },
      "source": [
        "This concatenated version is stored within the input_ids attribute of our Encoding object. But, rather than the human-readable text — the data is stored as BERT-readable token IDs.\n",
        "The tokenizer is great, but it doesn’t produce our answer start-end token positions. For that, we define a custom add_token_positions function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bXfYrIJlbE_"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    # initialize lists to contain the token indices of answer start/end\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        # append start/end token position using char_to_token method\n",
        "        start_positions.append(encodings.char_to_token(i, int(answers[i]['answer_start']))) #Casting str 'answer_start' to int\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "          # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        # end position cannot be found, char_to_token found space, so shift position until found\n",
        "        shift = 1\n",
        "        while end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
        "            shift += 1\n",
        "    # update our encodings object with the new token-based start/end positions\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "# apply function to our data\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6zBZ-WH-v7Q",
        "outputId": "7fcabce3-aa59-4064-948c-f60ddee2638f"
      },
      "source": [
        "train_encodings['start_positions'][0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FecNGX4T-3Q2",
        "outputId": "96d57091-5895-4640-e1c6-041e30a27b08"
      },
      "source": [
        "train_encodings['end_positions'][0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV08jICenkNe"
      },
      "source": [
        "##1.3 Initializing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtW82t_ZnjIG"
      },
      "source": [
        "import torch\n",
        "\n",
        "class TquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "# build datasets for both our training and validation sets\n",
        "train_dataset = TquadDataset(train_encodings)\n",
        "val_dataset = TquadDataset(val_encodings)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtXTF4RBnsTo"
      },
      "source": [
        "#2. Fine Tuning\n",
        "##2.1 Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s7Qj5P6nwVA",
        "outputId": "aa79f526-1d2b-4e94-81cd-b0586b3021f3"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# setup GPU/CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# move model over to detected device\n",
        "model.to(device)\n",
        "# activate training mode of model\n",
        "model.train()\n",
        "# initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
        "optim = AdamW(model.parameters(), lr=4e-5)\n",
        "\n",
        "# initialize data loader for training data\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "    # set model to train mode\n",
        "    model.train()\n",
        "    # setup loop (we use tqdm for the progress bar)\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all the tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        segment_ids = batch['token_type_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        # train model on batch and return outputs (incl. loss)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "        # extract loss\n",
        "        loss = outputs[0]\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 1039/1039 [14:50<00:00,  1.17it/s, loss=0.434]\n",
            "Epoch 1: 100%|██████████| 1039/1039 [14:50<00:00,  1.17it/s, loss=0.125]\n",
            "Epoch 2: 100%|██████████| 1039/1039 [14:50<00:00,  1.17it/s, loss=0.11]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcexysHeoIiZ"
      },
      "source": [
        "##2.2 Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lxt6e_v5oDTg",
        "outputId": "1578eb93-9981-4f3d-e737-1c234504e17b"
      },
      "source": [
        "model_path = \"models/tquad-custom3\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "#Use following code to load the model\n",
        "\"\"\"\n",
        "model = AutoModel.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel = AutoModel.from_pretrained(model_path)\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdLggusFkTSB",
        "outputId": "780db4a3-f1b6-4e92-fa1b-c812a0d849b7"
      },
      "source": [
        "!zip -r ./models.zip ./models/"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: models/ (stored 0%)\n",
            "  adding: models/tquad-custom3/ (stored 0%)\n",
            "  adding: models/tquad-custom3/tokenizer_config.json (deflated 34%)\n",
            "  adding: models/tquad-custom3/pytorch_model.bin (deflated 7%)\n",
            "  adding: models/tquad-custom3/vocab.txt (deflated 53%)\n",
            "  adding: models/tquad-custom3/special_tokens_map.json (deflated 40%)\n",
            "  adding: models/tquad-custom3/config.json (deflated 47%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "Acq_7YCQjXln",
        "outputId": "3d4a9000-8ee6-438f-e658-f8fad0a4b9c2"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"./models.zip\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9fb8b8ff-6fe1-450f-811d-2b656a43ea07\", \"models.zip\", 816406447)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLB4pLVoo29W"
      },
      "source": [
        "#3. Measuring Performance\n",
        "To extract the start-end token range from our model, we can access the start_logits and end_logits tensors and perform an argmax function like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VydcuuNCNjhB"
      },
      "source": [
        "start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "end_pred = torch.argmax(outputs['end_logits'], dim=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QibjFCv9OOZq"
      },
      "source": [
        "To calculate the EM of each batch, we take the sum of the number of matches per batch — and divide by the total. We do this with PyTorch like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04etMY1lNp2o"
      },
      "source": [
        "acc = ( (start_pred == start_positions).sum() / len(start_pred) ).item()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y5XujFDdnyg"
      },
      "source": [
        "## Use the information above to create an evaluation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQwWoAYKdboz"
      },
      "source": [
        "# switch model out of training mode\n",
        "model.eval()\n",
        "# initialize validation set data loader\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "# initialize list to store accuracies\n",
        "acc = []\n",
        "# loop through batches\n",
        "for batch in val_loader:\n",
        "    # we don't need to calculate gradients as we're not training\n",
        "    with torch.no_grad():\n",
        "        # pull batched items from loader\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        # we will use true positions for accuracy calc\n",
        "        start_true = batch['start_positions'].to(device)\n",
        "        end_true = batch['end_positions'].to(device)\n",
        "        # make predictions\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,)\n",
        "        # pull prediction tensors out and argmax to get predicted tokens\n",
        "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "        # calculate accuracy for both and append to accuracy list\n",
        "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
        "# calculate average accuracy in total\n",
        "acc = sum(acc)/len(acc)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l995NXbBeBlh",
        "outputId": "2d7cb9dd-a36e-4226-fc10-72a089c28ab9"
      },
      "source": [
        "acc"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6763392857142857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMoKv6DxfNL9"
      },
      "source": [
        "This accuracy seems a bit low. However, we can see that it predicts start and end points very closely if we look deeply."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adIoRQund58P",
        "outputId": "8d3c9078-b369-4235-f4a3-4d4b1b7b7a97"
      },
      "source": [
        "print(\"T/F\\tstart\\tend\\n\")\n",
        "for i in range(len(start_true)):\n",
        "    print(f\"true\\t{start_true[i]}\\t{end_true[i]}\\n\"\n",
        "          f\"pred\\t{start_pred[i]}\\t{end_pred[i]}\\n\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T/F\tstart\tend\n",
            "\n",
            "true\t212\t213\n",
            "pred\t212\t213\n",
            "\n",
            "true\t11\t16\n",
            "pred\t11\t16\n",
            "\n",
            "true\t11\t16\n",
            "pred\t11\t16\n",
            "\n",
            "true\t11\t16\n",
            "pred\t11\t16\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jjlPhUzeMNM"
      },
      "source": [
        "#4. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EkOoE2leL0B"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "#sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (16,8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkLbsvlngIHn"
      },
      "source": [
        "start_scores = outputs.start_logits[0]\n",
        "end_scores = outputs.end_logits[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAq9G6o_TTsh",
        "outputId": "63d92d8c-d11a-4e5e-b58b-048b824882c7"
      },
      "source": [
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(val_questions[0], val_contexts[0])\n",
        "\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The input has a total of 225 tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZz1UhDSTDwb",
        "outputId": "3a3ce3b2-2fc4-4156-dcd4-86505db7a76a"
      },
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its id...\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    \n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    \n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]             2\n",
            "Kemal         4,919\n",
            "##eddin      20,629\n",
            "ib            7,379\n",
            "##n           1,009\n",
            "Yunus        12,203\n",
            "lak          11,188\n",
            "##abı         3,421\n",
            "dışında       3,687\n",
            "hangi         4,161\n",
            "isimlerle    30,213\n",
            "bilinir      13,877\n",
            "?                35\n",
            "\n",
            "[SEP]             3\n",
            "\n",
            "Kemal         4,919\n",
            "##eddin      20,629\n",
            "ib            7,379\n",
            "##n           1,009\n",
            "Yunus        12,203\n",
            "ya            2,010\n",
            "da            2,054\n",
            "Musa         12,267\n",
            "ib            7,379\n",
            "##n           1,009\n",
            "Yunus        12,203\n",
            "(                12\n",
            "doğum         5,462\n",
            "yılı          4,399\n",
            "ve            1,992\n",
            "yeri          4,993\n",
            ":                30\n",
            "115          20,789\n",
            "##6           1,062\n",
            "Musul        22,334\n",
            "-                17\n",
            "ölüm          4,150\n",
            "yılı          4,399\n",
            "ve            1,992\n",
            "yeri          4,993\n",
            ":                30\n",
            "124          30,423\n",
            "##1           1,077\n",
            "Musul        22,334\n",
            ")                13\n",
            ".                18\n",
            "Astr         22,197\n",
            "##onom        2,896\n",
            ",                16\n",
            "matematik    10,205\n",
            "##çi          3,018\n",
            "ve            1,992\n",
            "İslam         4,384\n",
            "bilgi         3,129\n",
            "##ni          3,926\n",
            ".                18\n",
            "Tam           5,613\n",
            "adı           3,668\n",
            "Musa         12,267\n",
            "bin           2,551\n",
            "Yunus        12,203\n",
            "bin           2,551\n",
            "Muhammed      7,397\n",
            "bin           2,551\n",
            "Men           6,362\n",
            "’               554\n",
            "a                69\n",
            "'                11\n",
            "dır          12,018\n",
            ",                16\n",
            "Kü           16,917\n",
            "##ny         10,206\n",
            "##esi         2,061\n",
            "ise           2,342\n",
            "Ebu          12,745\n",
            "’               554\n",
            "l                80\n",
            "-                17\n",
            "Feth         12,961\n",
            "’               554\n",
            "tir          12,443\n",
            ",                16\n",
            "lak          11,188\n",
            "##abı         3,421\n",
            "Kemal         4,919\n",
            "##eddin      20,629\n",
            "olup          3,340\n",
            "ayrıca        4,146\n",
            "İbn          13,855\n",
            "-                17\n",
            "i                77\n",
            "Yunus        12,203\n",
            "ve            1,992\n",
            "Me            8,223\n",
            "##w           1,107\n",
            "##sil         3,344\n",
            "##î           1,089\n",
            "diye          2,636\n",
            "de            2,012\n",
            "bilinir      13,877\n",
            ".                18\n",
            "İlk           3,485\n",
            "eğitimini    15,194\n",
            "babası        7,217\n",
            "Şeyh         12,317\n",
            "Yunus        12,203\n",
            "Rıza         13,691\n",
            "##ud          4,488\n",
            "##din         3,112\n",
            "'                11\n",
            "in            2,091\n",
            "yanında       3,938\n",
            "fık           6,869\n",
            "##ıh         13,471\n",
            "ve            1,992\n",
            "hadis         9,090\n",
            "ilim         13,132\n",
            "##leri        2,065\n",
            "öğrendi      26,184\n",
            ",                16\n",
            "ardından      3,183\n",
            "Bağdat       12,656\n",
            "'                11\n",
            "taki          9,762\n",
            "Niz          23,272\n",
            "##ami         8,246\n",
            "##ye          2,736\n",
            "Medres       28,961\n",
            "##eleri       2,387\n",
            "'                11\n",
            "nde           3,871\n",
            "okumaya      15,940\n",
            "devam         2,570\n",
            "etti          2,534\n",
            ".                18\n",
            "Burada        4,458\n",
            "Şer           8,514\n",
            "##afe        12,967\n",
            "##d           1,005\n",
            "##din         3,112\n",
            "el            2,323\n",
            "-                17\n",
            "Tu           28,178\n",
            "##s           1,022\n",
            "##î           1,089\n",
            "'                11\n",
            "den           2,350\n",
            "matematik    10,205\n",
            "derslerin    26,476\n",
            "##i           1,024\n",
            "aldı          3,763\n",
            ",                16\n",
            "ardından      3,183\n",
            "Bat           5,903\n",
            "##lam         2,340\n",
            "##yu          7,005\n",
            "##s           1,022\n",
            "'                11\n",
            "un            2,621\n",
            "Alma         23,331\n",
            "##ges        14,795\n",
            "##t           1,023\n",
            "adlı          4,330\n",
            "eserin       22,327\n",
            "##i           1,024\n",
            "de            2,012\n",
            "öğrenir      23,167\n",
            ".                18\n",
            "Ardından      9,022\n",
            "Musul        22,334\n",
            "'                11\n",
            "a                69\n",
            "döndü         8,940\n",
            ",                16\n",
            "Emir         10,598\n",
            "Zeyn         11,984\n",
            "##eddin      20,629\n",
            "Camii        11,623\n",
            "'                11\n",
            "nde           3,871\n",
            "dersler      13,993\n",
            "verdi         3,815\n",
            ".                18\n",
            "İli          19,744\n",
            "##m           1,027\n",
            "öğret         3,142\n",
            "##meye        2,646\n",
            "elverişli    19,103\n",
            "olarak        2,133\n",
            "inşa          4,278\n",
            "edilen        3,166\n",
            "bu            2,048\n",
            "cami          8,600\n",
            "Kemal         4,919\n",
            "##iyye       19,424\n",
            "Medres       28,961\n",
            "##esi         2,061\n",
            "olarak        2,133\n",
            "anı           7,964\n",
            "##ld         10,106\n",
            "##ı           1,048\n",
            ".                18\n",
            "Kısa          8,147\n",
            "zamanda       3,419\n",
            "şöhret       20,513\n",
            "##i           1,024\n",
            "etrafa       26,857\n",
            "yayılan      18,148\n",
            "Musa         12,267\n",
            "Kemal         4,919\n",
            "##eddin      20,629\n",
            "ib            7,379\n",
            "##n           1,009\n",
            "Yunus        12,203\n",
            "pek           3,312\n",
            "çok           2,140\n",
            "çevrede      25,132\n",
            "##n           1,009\n",
            "gelen         2,665\n",
            "taleb         7,165\n",
            "##elere       3,565\n",
            "ilim         13,132\n",
            "öğret         3,142\n",
            "##ti          2,063\n",
            ".                18\n",
            "\n",
            "[SEP]             3\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l35OJNaBfn4C"
      },
      "source": [
        "# Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays.\n",
        "s_scores = start_scores.cpu().detach().numpy().flatten()\n",
        "e_scores = end_scores.cpu().detach().numpy().flatten()\n",
        "\n",
        "# We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
        "# to be unique, so we'll add the token index to the end of each one.\n",
        "token_labels = []\n",
        "for (i, token) in enumerate(tokens):\n",
        "    token_labels.append('{:} - {:>2}'.format(token, i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVuWpRSSSbAx",
        "outputId": "714dabc7-4913-4924-bf7d-be5930b4c902"
      },
      "source": [
        "token_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] -  0',\n",
              " 'Kemal -  1',\n",
              " '##eddin -  2',\n",
              " 'ib -  3',\n",
              " '##n -  4',\n",
              " 'Yunus -  5',\n",
              " 'lak -  6',\n",
              " '##abı -  7',\n",
              " 'dışında -  8',\n",
              " 'hangi -  9',\n",
              " 'isimlerle - 10',\n",
              " 'bilinir - 11',\n",
              " '? - 12',\n",
              " '[SEP] - 13',\n",
              " 'Kemal - 14',\n",
              " '##eddin - 15',\n",
              " 'ib - 16',\n",
              " '##n - 17',\n",
              " 'Yunus - 18',\n",
              " 'ya - 19',\n",
              " 'da - 20',\n",
              " 'Musa - 21',\n",
              " 'ib - 22',\n",
              " '##n - 23',\n",
              " 'Yunus - 24',\n",
              " '( - 25',\n",
              " 'doğum - 26',\n",
              " 'yılı - 27',\n",
              " 've - 28',\n",
              " 'yeri - 29',\n",
              " ': - 30',\n",
              " '115 - 31',\n",
              " '##6 - 32',\n",
              " 'Musul - 33',\n",
              " '- - 34',\n",
              " 'ölüm - 35',\n",
              " 'yılı - 36',\n",
              " 've - 37',\n",
              " 'yeri - 38',\n",
              " ': - 39',\n",
              " '124 - 40',\n",
              " '##1 - 41',\n",
              " 'Musul - 42',\n",
              " ') - 43',\n",
              " '. - 44',\n",
              " 'Astr - 45',\n",
              " '##onom - 46',\n",
              " ', - 47',\n",
              " 'matematik - 48',\n",
              " '##çi - 49',\n",
              " 've - 50',\n",
              " 'İslam - 51',\n",
              " 'bilgi - 52',\n",
              " '##ni - 53',\n",
              " '. - 54',\n",
              " 'Tam - 55',\n",
              " 'adı - 56',\n",
              " 'Musa - 57',\n",
              " 'bin - 58',\n",
              " 'Yunus - 59',\n",
              " 'bin - 60',\n",
              " 'Muhammed - 61',\n",
              " 'bin - 62',\n",
              " 'Men - 63',\n",
              " '’ - 64',\n",
              " 'a - 65',\n",
              " \"' - 66\",\n",
              " 'dır - 67',\n",
              " ', - 68',\n",
              " 'Kü - 69',\n",
              " '##ny - 70',\n",
              " '##esi - 71',\n",
              " 'ise - 72',\n",
              " 'Ebu - 73',\n",
              " '’ - 74',\n",
              " 'l - 75',\n",
              " '- - 76',\n",
              " 'Feth - 77',\n",
              " '’ - 78',\n",
              " 'tir - 79',\n",
              " ', - 80',\n",
              " 'lak - 81',\n",
              " '##abı - 82',\n",
              " 'Kemal - 83',\n",
              " '##eddin - 84',\n",
              " 'olup - 85',\n",
              " 'ayrıca - 86',\n",
              " 'İbn - 87',\n",
              " '- - 88',\n",
              " 'i - 89',\n",
              " 'Yunus - 90',\n",
              " 've - 91',\n",
              " 'Me - 92',\n",
              " '##w - 93',\n",
              " '##sil - 94',\n",
              " '##î - 95',\n",
              " 'diye - 96',\n",
              " 'de - 97',\n",
              " 'bilinir - 98',\n",
              " '. - 99',\n",
              " 'İlk - 100',\n",
              " 'eğitimini - 101',\n",
              " 'babası - 102',\n",
              " 'Şeyh - 103',\n",
              " 'Yunus - 104',\n",
              " 'Rıza - 105',\n",
              " '##ud - 106',\n",
              " '##din - 107',\n",
              " \"' - 108\",\n",
              " 'in - 109',\n",
              " 'yanında - 110',\n",
              " 'fık - 111',\n",
              " '##ıh - 112',\n",
              " 've - 113',\n",
              " 'hadis - 114',\n",
              " 'ilim - 115',\n",
              " '##leri - 116',\n",
              " 'öğrendi - 117',\n",
              " ', - 118',\n",
              " 'ardından - 119',\n",
              " 'Bağdat - 120',\n",
              " \"' - 121\",\n",
              " 'taki - 122',\n",
              " 'Niz - 123',\n",
              " '##ami - 124',\n",
              " '##ye - 125',\n",
              " 'Medres - 126',\n",
              " '##eleri - 127',\n",
              " \"' - 128\",\n",
              " 'nde - 129',\n",
              " 'okumaya - 130',\n",
              " 'devam - 131',\n",
              " 'etti - 132',\n",
              " '. - 133',\n",
              " 'Burada - 134',\n",
              " 'Şer - 135',\n",
              " '##afe - 136',\n",
              " '##d - 137',\n",
              " '##din - 138',\n",
              " 'el - 139',\n",
              " '- - 140',\n",
              " 'Tu - 141',\n",
              " '##s - 142',\n",
              " '##î - 143',\n",
              " \"' - 144\",\n",
              " 'den - 145',\n",
              " 'matematik - 146',\n",
              " 'derslerin - 147',\n",
              " '##i - 148',\n",
              " 'aldı - 149',\n",
              " ', - 150',\n",
              " 'ardından - 151',\n",
              " 'Bat - 152',\n",
              " '##lam - 153',\n",
              " '##yu - 154',\n",
              " '##s - 155',\n",
              " \"' - 156\",\n",
              " 'un - 157',\n",
              " 'Alma - 158',\n",
              " '##ges - 159',\n",
              " '##t - 160',\n",
              " 'adlı - 161',\n",
              " 'eserin - 162',\n",
              " '##i - 163',\n",
              " 'de - 164',\n",
              " 'öğrenir - 165',\n",
              " '. - 166',\n",
              " 'Ardından - 167',\n",
              " 'Musul - 168',\n",
              " \"' - 169\",\n",
              " 'a - 170',\n",
              " 'döndü - 171',\n",
              " ', - 172',\n",
              " 'Emir - 173',\n",
              " 'Zeyn - 174',\n",
              " '##eddin - 175',\n",
              " 'Camii - 176',\n",
              " \"' - 177\",\n",
              " 'nde - 178',\n",
              " 'dersler - 179',\n",
              " 'verdi - 180',\n",
              " '. - 181',\n",
              " 'İli - 182',\n",
              " '##m - 183',\n",
              " 'öğret - 184',\n",
              " '##meye - 185',\n",
              " 'elverişli - 186',\n",
              " 'olarak - 187',\n",
              " 'inşa - 188',\n",
              " 'edilen - 189',\n",
              " 'bu - 190',\n",
              " 'cami - 191',\n",
              " 'Kemal - 192',\n",
              " '##iyye - 193',\n",
              " 'Medres - 194',\n",
              " '##esi - 195',\n",
              " 'olarak - 196',\n",
              " 'anı - 197',\n",
              " '##ld - 198',\n",
              " '##ı - 199',\n",
              " '. - 200',\n",
              " 'Kısa - 201',\n",
              " 'zamanda - 202',\n",
              " 'şöhret - 203',\n",
              " '##i - 204',\n",
              " 'etrafa - 205',\n",
              " 'yayılan - 206',\n",
              " 'Musa - 207',\n",
              " 'Kemal - 208',\n",
              " '##eddin - 209',\n",
              " 'ib - 210',\n",
              " '##n - 211',\n",
              " 'Yunus - 212',\n",
              " 'pek - 213',\n",
              " 'çok - 214',\n",
              " 'çevrede - 215',\n",
              " '##n - 216',\n",
              " 'gelen - 217',\n",
              " 'taleb - 218',\n",
              " '##elere - 219',\n",
              " 'ilim - 220',\n",
              " 'öğret - 221',\n",
              " '##ti - 222',\n",
              " '. - 223',\n",
              " '[SEP] - 224']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqXz4Mk-hPu3",
        "outputId": "f8058bcd-feb6-45f7-ee52-f37d979d03f5"
      },
      "source": [
        "s_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-11.038198  , -10.696846  , -11.488611  , -11.220735  ,\n",
              "       -11.439271  , -11.519564  , -11.530981  , -10.760305  ,\n",
              "        -6.2946334 ,  -8.088298  ,  -6.631877  , -11.4611435 ,\n",
              "        -9.24194   ,  -8.145511  , -11.071815  , -11.795953  ,\n",
              "       -11.966558  , -10.867318  , -10.353082  ,  -2.2200944 ,\n",
              "        -7.7910013 , -10.213983  , -12.199917  , -11.375211  ,\n",
              "       -12.24846   , -12.22295   , -11.680821  , -11.783639  ,\n",
              "       -11.030941  ,  -8.840407  , -11.249954  ,  -9.720007  ,\n",
              "       -12.005323  , -11.427193  , -11.784488  , -11.55848   ,\n",
              "       -11.971197  , -11.785428  , -11.012595  , -10.0418    ,\n",
              "        -3.9203193 ,  -9.541165  ,  -9.388438  , -10.475455  ,\n",
              "       -11.746798  , -11.197577  , -10.65748   , -10.0265465 ,\n",
              "       -12.137065  , -11.4524555 , -11.886969  , -11.937933  ,\n",
              "       -11.952536  , -11.573404  , -11.81254   , -11.738305  ,\n",
              "       -10.927056  , -10.285285  ,  -3.1101096 ,  -8.785028  ,\n",
              "       -10.149656  , -11.729133  , -11.913747  , -11.758182  ,\n",
              "       -11.957931  , -11.835572  , -12.067677  , -11.864855  ,\n",
              "       -11.794064  , -11.633939  , -10.415528  , -11.586106  ,\n",
              "       -11.410694  , -10.512931  ,  -9.536264  ,  -3.902975  ,\n",
              "       -10.117331  ,  -3.26668   ,  -9.159359  ,  -8.658863  ,\n",
              "        -6.077156  ,  -9.932617  , -11.801518  , -11.695489  ,\n",
              "       -11.547085  , -10.761075  , -11.98884   , -11.53712   ,\n",
              "       -10.7189245 ,  -9.190954  ,  -7.2303762 ,  -6.417699  ,\n",
              "        -6.930821  , -10.866698  , -11.572028  , -11.953294  ,\n",
              "       -12.27209   , -11.947673  , -11.599886  , -12.300223  ,\n",
              "       -10.690849  , -10.035926  ,   0.6643167 ,  -9.942478  ,\n",
              "         0.6928068 ,  -4.1794043 ,  -6.9745235 , -11.997375  ,\n",
              "       -11.118131  , -10.215979  , -11.952983  , -12.167064  ,\n",
              "       -12.521357  , -12.125544  , -11.795915  , -11.670476  ,\n",
              "       -11.338949  ,  -9.685772  ,  -8.556883  ,  -1.1867753 ,\n",
              "        -8.405426  ,  -7.807138  ,  -9.5300255 , -11.468377  ,\n",
              "       -11.002348  , -10.831495  , -11.627269  , -11.018627  ,\n",
              "       -11.818325  , -11.886953  , -11.825705  , -11.327831  ,\n",
              "       -11.599281  , -11.842759  , -12.047992  , -11.496144  ,\n",
              "       -11.370008  , -12.104981  , -12.565148  , -11.773742  ,\n",
              "       -11.796992  , -11.4795885 , -11.554063  , -11.879688  ,\n",
              "       -11.527391  , -11.654968  , -11.85249   , -11.839285  ,\n",
              "       -11.709545  , -11.299098  , -10.972299  , -11.564346  ,\n",
              "       -11.563992  , -11.505774  , -11.405352  , -10.47035   ,\n",
              "        -9.554312  , -11.7887945 , -11.506062  , -11.342915  ,\n",
              "       -11.8452015 , -11.8806095 , -11.740988  , -11.451862  ,\n",
              "       -11.900874  , -11.939395  , -11.1733055 ,  -9.246752  ,\n",
              "         0.2745778 , -10.630566  ,  -1.1150584 ,  -8.912596  ,\n",
              "        -7.528934  ,  -4.133292  ,  -8.446351  , -12.134701  ,\n",
              "       -11.868654  , -11.488865  , -11.879485  , -10.803258  ,\n",
              "       -11.169836  ,  -1.7337738 ,  -7.812651  ,  -7.0961065 ,\n",
              "        -8.659164  , -11.984505  , -12.404783  , -12.258943  ,\n",
              "       -10.408863  , -11.3847065 , -10.874408  , -11.325451  ,\n",
              "       -11.634025  , -10.323807  ,  -8.430237  ,   2.7062826 ,\n",
              "        -7.375602  ,  -4.6031284 ,  -3.7561905 , -12.0942    ,\n",
              "       -12.481768  , -11.708791  ,  -7.9906673 , -10.931495  ,\n",
              "       -10.060582  ,  -8.017493  ,   0.27023113,  -7.1508183 ,\n",
              "        -7.311193  , -11.694932  ,  -8.314831  ,  -6.261569  ,\n",
              "         9.832399  ,   0.32223418,  -4.213663  , -10.735202  ,\n",
              "       -10.159357  ,  -9.596964  ,  -8.824905  , -10.623334  ,\n",
              "       -10.060839  ,  -8.747993  ,  -9.800316  ,  -3.4391286 ,\n",
              "        -7.083247  , -11.164183  , -11.575996  , -11.575878  ,\n",
              "       -11.51021   , -11.744842  , -11.845034  , -11.208198  ,\n",
              "       -11.448799  , -11.282871  , -11.986466  , -11.124404  ,\n",
              "       -10.463986  , -12.126823  , -11.943364  , -12.162119  ,\n",
              "       -11.357905  ,  -9.547711  ,  -1.6581162 , -11.086705  ,\n",
              "        -5.883481  ,  -8.60891   ,  -6.343793  , -11.151967  ,\n",
              "       -12.805594  , -12.276161  , -12.8332615 , -12.969271  ,\n",
              "       -12.71681   , -11.4366    ,  -1.1326557 ,  -9.56255   ,\n",
              "        -6.4304833 ,  -7.165791  ,  -7.8507967 ,  -9.441291  ,\n",
              "       -12.070707  , -12.456594  , -12.013835  , -11.453411  ,\n",
              "       -10.961498  , -12.11676   , -11.1810875 , -11.433391  ,\n",
              "       -11.72973   , -11.255166  , -10.852213  ,  -9.4146595 ,\n",
              "        -9.913558  , -11.460611  , -11.709065  , -11.839523  ,\n",
              "       -11.863361  , -11.656794  , -11.761627  , -12.152893  ,\n",
              "       -11.952103  , -11.072887  , -11.268183  , -12.241748  ,\n",
              "       -11.734101  , -11.740252  , -11.470712  , -11.403385  ,\n",
              "       -11.251681  , -11.646749  , -11.526921  , -11.613238  ,\n",
              "       -11.911646  , -11.523155  , -10.861021  , -11.639048  ,\n",
              "        -9.409628  , -12.656363  , -12.344963  , -11.999944  ,\n",
              "       -12.192293  ,  -9.900866  ,  -7.162921  , -12.567308  ,\n",
              "        -9.88706   , -11.032658  , -11.999535  ,  -9.47284   ,\n",
              "       -11.652511  , -11.933223  , -11.894203  , -11.999031  ,\n",
              "       -11.898977  , -11.874581  , -11.757687  , -11.687303  ,\n",
              "       -11.863665  , -12.0422125 , -12.030663  , -11.991614  ,\n",
              "       -11.70574   , -11.811329  , -11.623828  , -11.652952  ,\n",
              "       -11.643716  , -11.585072  ,  -9.472683  , -11.557623  ,\n",
              "       -11.55567   , -11.558051  , -11.560954  , -11.561877  ,\n",
              "       -11.560414  , -11.559653  , -11.559323  , -11.562967  ,\n",
              "       -11.567095  , -11.566705  , -11.56572   , -11.571564  ,\n",
              "       -11.575018  , -11.571334  , -11.57086   , -11.569773  ,\n",
              "       -11.568936  , -11.569923  , -11.571072  , -11.568957  ,\n",
              "       -11.572325  , -11.568445  , -11.577443  , -11.570431  ,\n",
              "       -11.569173  , -11.571343  , -11.571492  , -11.606295  ,\n",
              "       -11.587688  , -11.625139  , -11.613134  , -11.611696  ,\n",
              "       -11.598352  , -11.608725  , -11.618828  , -11.609805  ,\n",
              "       -11.612819  , -11.623456  , -11.604946  , -11.596454  ,\n",
              "       -11.607933  , -11.613476  , -11.586685  , -11.586628  ,\n",
              "       -11.587067  , -11.600663  , -11.606745  , -11.598236  ,\n",
              "       -11.617688  , -11.597555  , -11.621887  , -11.601602  ,\n",
              "       -11.617732  , -11.6032715 , -11.605948  , -11.634672  ,\n",
              "       -11.591489  , -11.589784  , -11.587747  , -11.588829  ,\n",
              "       -11.578955  , -11.580266  , -11.584826  , -11.585414  ,\n",
              "       -11.587315  , -11.580553  , -11.582889  , -11.578952  ,\n",
              "       -11.584499  , -11.624189  , -11.595642  , -11.592945  ,\n",
              "       -11.588857  , -11.631551  , -11.664331  , -11.686531  ,\n",
              "       -11.698874  , -11.681906  , -11.686393  , -11.674318  ,\n",
              "       -11.6858835 , -11.693796  , -11.7541685 , -11.715021  ,\n",
              "       -11.730898  , -11.695425  , -11.702411  , -11.728411  ,\n",
              "       -11.695835  , -11.677633  , -11.683284  , -11.660349  ,\n",
              "       -11.660168  , -11.643478  , -11.661602  , -11.649326  ,\n",
              "       -11.659424  , -11.635201  , -11.65402   , -11.672593  ,\n",
              "       -11.6690445 , -11.6674595 , -11.674965  , -11.668402  ,\n",
              "       -11.671766  , -11.667667  , -11.664148  , -11.702548  ,\n",
              "       -11.685375  , -11.685536  , -11.672188  , -11.688193  ,\n",
              "       -11.677289  , -11.707388  , -11.707248  , -11.666226  ,\n",
              "       -11.679779  , -11.629776  , -11.645828  , -11.659367  ,\n",
              "       -11.694838  , -11.621209  , -11.606933  , -11.580846  ,\n",
              "       -11.586742  , -11.580683  , -11.589487  , -11.580693  ,\n",
              "       -11.583515  , -11.578232  , -11.577605  , -11.582076  ,\n",
              "       -11.592251  , -11.5865    , -11.592605  , -11.594527  ,\n",
              "       -11.590847  , -11.599874  , -11.605743  , -11.605856  ,\n",
              "       -11.59623   , -11.600563  , -11.598949  , -11.607037  ,\n",
              "       -11.598962  , -11.619388  , -11.598507  , -11.606343  ,\n",
              "       -11.62001   , -11.637893  , -11.636631  , -11.662107  ,\n",
              "       -11.656427  , -11.665564  , -11.645784  , -11.653976  ,\n",
              "       -11.649946  , -11.654328  , -11.63591   , -11.639829  ,\n",
              "       -11.632964  , -11.637208  , -11.640421  , -11.655251  ,\n",
              "       -11.664398  , -11.689432  , -11.670693  , -11.673201  ,\n",
              "       -11.665817  , -11.667273  , -11.673464  , -11.675973  ,\n",
              "       -11.677673  , -11.668625  , -11.646988  , -11.633682  ,\n",
              "       -11.633159  , -11.624995  , -11.62121   , -11.612064  ,\n",
              "       -11.614052  , -11.602421  , -11.5899515 ,  -9.517832  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "oB-AaPW1g3b2",
        "outputId": "5e02559b-f3c8-4681-ed1b-628f999465f4"
      },
      "source": [
        "# Create a barplot showing the start word score for all of the tokens.\n",
        "ax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "ax.grid(True)\n",
        "\n",
        "plt.title('Start Word Scores')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-04dc520c223e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a barplot showing the start word score for all of the tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Turn the xlabels vertical.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"center\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[0;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   3180\u001b[0m                           \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3181\u001b[0m                           \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3182\u001b[0;31m                           errcolor, errwidth, capsize, dodge)\n\u001b[0m\u001b[1;32m   3183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0;32m-> 1585\u001b[0;31m                                  order, hue_order, units)\n\u001b[0m\u001b[1;32m   1586\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_statistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;31m# Group the numeric data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 plot_data, value_label = self._group_longform(vals, groups,\n\u001b[0;32m--> 207\u001b[0;31m                                                               group_names)\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;31m# Now handle the hue levels for nested ordering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m_group_longform\u001b[0;34m(self, vals, grouper, order)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Group the val data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mgrouped_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   1744\u001b[0m             \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1746\u001b[0;31m             \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1747\u001b[0m         )\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0mmutated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                 \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m             )\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '[CLS] -  0'"
          ]
        }
      ]
    }
  ]
}
